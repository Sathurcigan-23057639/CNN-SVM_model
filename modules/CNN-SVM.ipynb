{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f7978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 validated image filenames belonging to 4 classes.\n",
      "Found 60 validated image filenames belonging to 4 classes.\n",
      "Training CNN on MESSIDOR dataset...\n",
      "Epoch 1/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3679 - loss: 1.5101\n",
      "Epoch 1: val_loss improved from None to 1.27156, saving model to ./outputs\\best_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.3292 - loss: 1.6004 - val_accuracy: 0.4667 - val_loss: 1.2716\n",
      "Epoch 2/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3604 - loss: 1.2967\n",
      "Epoch 2: val_loss did not improve from 1.27156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.4167 - loss: 1.2325 - val_accuracy: 0.4667 - val_loss: 1.2989\n",
      "Epoch 3/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6313 - loss: 0.9136\n",
      "Epoch 3: val_loss did not improve from 1.27156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.5875 - loss: 0.9401 - val_accuracy: 0.4667 - val_loss: 1.3178\n",
      "Epoch 4/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6313 - loss: 0.7313\n",
      "Epoch 4: val_loss did not improve from 1.27156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - accuracy: 0.6667 - loss: 0.7672 - val_accuracy: 0.4667 - val_loss: 1.2735\n",
      "Epoch 5/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7156 - loss: 0.5721\n",
      "Epoch 5: val_loss did not improve from 1.27156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.7250 - loss: 0.6133 - val_accuracy: 0.3000 - val_loss: 1.4207\n",
      "Epoch 6/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8596 - loss: 0.3687\n",
      "Epoch 6: val_loss did not improve from 1.27156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - accuracy: 0.8583 - loss: 0.4218 - val_accuracy: 0.3000 - val_loss: 1.8365\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CNN saved at: ./outputs\\final_cnn_model.h5\n",
      "CNN feature extractor saved at: ./outputs\\cnn_feature_extractor.h5\n",
      "SVM features shape: (300, 512)\n",
      "\n",
      "--- Training SVM on Updated Features (512-D) ---\n",
      "SVM training completed. PCA output dim: 30\n",
      "\n",
      "Fusion Metrics on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        18\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.82      0.82      0.82        28\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.35      0.46      0.39        60\n",
      "weighted avg       0.55      0.68      0.60        60\n",
      "\n",
      "Cohen's kappa: 0.4910714285714286\n",
      "Using dataset: MESSIDOR\n",
      "\n",
      "--- Training CNN ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4306 - loss: 1.2537\n",
      "Epoch 1: val_loss improved from 1.27156 to 1.25882, saving model to ./outputs\\best_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - accuracy: 0.4792 - loss: 1.2084 - val_accuracy: 0.4667 - val_loss: 1.2588\n",
      "Epoch 2/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6126 - loss: 0.9909\n",
      "Epoch 2: val_loss improved from 1.25882 to 1.25005, saving model to ./outputs\\best_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.5750 - loss: 0.9548 - val_accuracy: 0.4667 - val_loss: 1.2500\n",
      "Epoch 3/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6932 - loss: 0.7165\n",
      "Epoch 3: val_loss improved from 1.25005 to 1.24618, saving model to ./outputs\\best_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.6958 - loss: 0.7703 - val_accuracy: 0.4667 - val_loss: 1.2462\n",
      "Epoch 4/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7844 - loss: 0.4923\n",
      "Epoch 4: val_loss improved from 1.24618 to 1.22949, saving model to ./outputs\\best_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.7917 - loss: 0.4979 - val_accuracy: 0.4667 - val_loss: 1.2295\n",
      "Epoch 5/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7888 - loss: 0.4253\n",
      "Epoch 5: val_loss did not improve from 1.22949\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.8417 - loss: 0.3693 - val_accuracy: 0.3000 - val_loss: 1.2810\n",
      "Epoch 6/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8435 - loss: 0.2807\n",
      "Epoch 6: val_loss did not improve from 1.22949\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.8375 - loss: 0.3247 - val_accuracy: 0.3000 - val_loss: 1.4997\n",
      "Epoch 7/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8589 - loss: 0.2845\n",
      "Epoch 7: val_loss did not improve from 1.22949\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.8583 - loss: 0.3001 - val_accuracy: 0.3000 - val_loss: 1.5850\n",
      "Epoch 8/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8434 - loss: 0.2495\n",
      "Epoch 8: val_loss did not improve from 1.22949\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.8500 - loss: 0.2631 - val_accuracy: 0.3000 - val_loss: 1.7601\n",
      "Epoch 9/20\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9336 - loss: 0.1597\n",
      "Epoch 9: val_loss did not improve from 1.22949\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.9375 - loss: 0.1592 - val_accuracy: 0.3000 - val_loss: 1.6253\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CNN model saved: ./outputs\\final_cnn_model.h5\n",
      "\n",
      "--- Extracting CNN features ---\n",
      "CNN feature extractor saved: ./outputs\\cnn_feature_extractor.h5\n",
      "SVM feature matrix shape: (300, 512), labels shape: (300,)\n",
      "\n",
      "--- Training SVM ---\n",
      "SVM training finished in 0.05 seconds\n",
      "\n",
      "--- SVM Training Metrics ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       154\n",
      "           1       1.00      1.00      1.00        36\n",
      "           2       1.00      1.00      1.00        45\n",
      "           3       0.98      1.00      0.99        65\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       1.00      1.00      1.00       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n",
      "Cohen's kappa: 0.9949003008822479\n",
      "Saved SVM artifacts.\n",
      "\n",
      "--- Fusion CNN + SVM ---\n",
      "\n",
      "Fusion Metrics on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      1.00      0.46        18\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.30        60\n",
      "   macro avg       0.07      0.25      0.12        60\n",
      "weighted avg       0.09      0.30      0.14        60\n",
      "\n",
      "Cohen's kappa: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN-SVM Hybrid for Diabetic Retinopathy Classification\n",
    "- Supports Kaggle and Messidor (Messidor has 3 CSVs)\n",
    "- Uses ImageDataGenerator to avoid memory overflow\n",
    "- CNN feature extraction + SVM on penultimate features\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "USE_DATASET = \"MESSIDOR\"  # <== Options: \"KAGGLE\" or \"MESSIDOR\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "KAGGLE_IMAGES_DIR = r\"D:/Education/MSc/Active Assignments/Project/Model/KDR_Pre-processed/subset\"\n",
    "KAGGLE_CSV = r\"D:/Education/MSc/Active Assignments/Project/Model/KDR_Pre-processed/subset/subset_labels.csv\"\n",
    "\n",
    "MESSIDOR_IMAGES_DIR = r\"D:/Education/MSc/Active Assignments/Project/Model/MS_Pre-processed\"\n",
    "MESSIDOR_CSVS = [\n",
    "    r\"D:/Education/MSc/Active Assignments/Project/Model/MS_Pre-processed/Annotation_Base11.csv\",\n",
    "    r\"D:/Education/MSc/Active Assignments/Project/Model/MS_Pre-processed/Annotation_Base21.csv\",\n",
    "    r\"D:/Education/MSc/Active Assignments/Project/Model/MS_Pre-processed/Annotation_Base31.csv\"\n",
    "]\n",
    "\n",
    "SVM_MODEL_PATH = os.path.join(OUTPUT_DIR, \"svm_classifier.pkl\")\n",
    "PCA_PATH = os.path.join(OUTPUT_DIR, \"pca_transform.pkl\")\n",
    "SCALER_PATH = os.path.join(OUTPUT_DIR, \"scaler_transform.pkl\")\n",
    "CNN_FEATURE_EXTRACTOR_PATH = os.path.join(OUTPUT_DIR, \"cnn_feature_extractor.h5\")\n",
    "BEST_CNN_CHECKPOINT = os.path.join(OUTPUT_DIR, \"best_cnn.h5\")\n",
    "FINAL_CNN_MODEL_PATH = os.path.join(OUTPUT_DIR, \"final_cnn_model.h5\")\n",
    "\n",
    "# ----------------------------\n",
    "# DATA LOADING\n",
    "# ----------------------------\n",
    "def load_kaggle(csv_path, img_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    df = df.rename(columns={\"image\": \"file_path\", \"level\": \"raw_label\"})\n",
    "    df['file_path'] = df['file_path'].apply(lambda x: os.path.join(img_dir, str(x)))\n",
    "    df = df[df['file_path'].apply(os.path.exists)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_messidor(csv_paths, img_dir):\n",
    "    dfs = []\n",
    "    for path in csv_paths:\n",
    "        df_temp = pd.read_csv(path)\n",
    "        df_temp.columns = [c.strip().lower() for c in df_temp.columns]\n",
    "        df_temp = df_temp.rename(columns={\"image name\": \"file_path\", \"retinopathy grade\": \"raw_label\"})\n",
    "        df_temp['file_path'] = df_temp['file_path'].apply(lambda x: os.path.join(img_dir, str(x)))\n",
    "        dfs.append(df_temp)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df = df[df['file_path'].apply(os.path.exists)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "if USE_DATASET == \"KAGGLE\":\n",
    "    df = load_kaggle(KAGGLE_CSV, KAGGLE_IMAGES_DIR)\n",
    "elif USE_DATASET == \"MESSIDOR\":\n",
    "    df = load_messidor(MESSIDOR_CSVS, MESSIDOR_IMAGES_DIR)\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset selection!\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['raw_label'].astype(int))\n",
    "df['label_str'] = df['label_encoded'].astype(str)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# ----------------------------\n",
    "# IMAGE GENERATOR\n",
    "# ----------------------------\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    brightness_range=[0.9, 1.1]\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(\n",
    "    df,\n",
    "    x_col='file_path',\n",
    "    y_col='label_str',      # use string labels\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(\n",
    "    df,\n",
    "    x_col='file_path',\n",
    "    y_col='label_str',      # use string labels\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CLASS WEIGHTS\n",
    "# ----------------------------\n",
    "cw = class_weight.compute_class_weight('balanced', classes=np.unique(train_gen.classes), y=train_gen.classes)\n",
    "class_weights_dict = dict(zip(np.unique(train_gen.classes), cw))\n",
    "\n",
    "# ----------------------------\n",
    "# CNN MODEL\n",
    "# ----------------------------\n",
    "def build_resnet50_head(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=5, penultimate_units=512):\n",
    "    base = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D(name=\"gap\")(base.output)\n",
    "    x = Dense(penultimate_units, activation=\"relu\", name=\"penultimate_dense\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', name='softmax_head')(x)\n",
    "    model = Model(inputs=base.input, outputs=outputs, name=\"ResNet50_with_head\")\n",
    "    return model\n",
    "\n",
    "def feature_extractor(model):\n",
    "    penultimate = model.get_layer(\"penultimate_dense\").output\n",
    "    return Model(inputs=model.input, outputs=penultimate)\n",
    "\n",
    "# ----------------------------\n",
    "# TRAIN CNN\n",
    "# ----------------------------\n",
    "model = build_resnet50_head(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=num_classes)\n",
    "model.compile(optimizer=Adam(LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(BEST_CNN_CHECKPOINT, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "steps_per_epoch = math.ceil(train_gen.n / BATCH_SIZE)\n",
    "validation_steps = math.ceil(val_gen.n / BATCH_SIZE)\n",
    "\n",
    "print(f\"Training CNN on {USE_DATASET} dataset...\")\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "model.save(FINAL_CNN_MODEL_PATH)\n",
    "print(\"Final CNN saved at:\", FINAL_CNN_MODEL_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE EXTRACTION\n",
    "# ----------------------------\n",
    "feat_model = feature_extractor(model)\n",
    "feat_model.save(CNN_FEATURE_EXTRACTOR_PATH)\n",
    "print(\"CNN feature extractor saved at:\", CNN_FEATURE_EXTRACTOR_PATH)\n",
    "\n",
    "def extract_features_in_batches(feat_model, data_gen):\n",
    "    X_list, y_list = [], []\n",
    "    steps = math.ceil(data_gen.n / data_gen.batch_size)\n",
    "    for i in range(steps):\n",
    "        X_batch, y_batch = next(data_gen)\n",
    "        feats = feat_model.predict(X_batch, verbose=0)\n",
    "        X_list.append(feats)\n",
    "        y_list.append(np.argmax(y_batch, axis=1))\n",
    "    X_feat = np.vstack(X_list)\n",
    "    y_feat = np.concatenate(y_list)\n",
    "    return X_feat, y_feat\n",
    "\n",
    "# Reset generator for feature extraction\n",
    "train_gen.reset()\n",
    "X_train_feat, y_train_feat = extract_features_in_batches(feat_model, train_gen)\n",
    "val_gen.reset()\n",
    "X_val_feat, y_val_feat = extract_features_in_batches(feat_model, val_gen)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TRAIN SVM\n",
    "# ----------------------------\n",
    "X_svm = np.vstack([X_train_feat, X_val_feat])\n",
    "y_svm = np.concatenate([y_train_feat, y_val_feat])\n",
    "print(\"SVM features shape:\", X_svm.shape)\n",
    "\n",
    "print(\"\\n--- Training SVM on Updated Features (512-D) ---\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_svm)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', probability=True)\n",
    "svm_model.fit(X_pca, y_svm)\n",
    "\n",
    "print(\"SVM training completed. PCA output dim:\", X_pca.shape[1])\n",
    "\n",
    "# Save only these\n",
    "joblib.dump(svm_model, SVM_MODEL_PATH)\n",
    "joblib.dump(pca, PCA_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FUSION PREDICTION FUNCTION\n",
    "# ----------------------------\n",
    "def fusion_predict_from_features(cnn_model, svm_model, pca, scaler, X_val_feat, val_labels, val_gen, batch_size=16):\n",
    "    # CNN predictions on validation images\n",
    "    val_gen.reset()\n",
    "    steps = math.ceil(val_gen.n / batch_size)\n",
    "    cnn_probs_list = []\n",
    "    for i in range(steps):\n",
    "        X_batch, _ = next(val_gen)\n",
    "        cnn_probs_batch = cnn_model.predict(X_batch, verbose=0)\n",
    "        cnn_probs_list.append(cnn_probs_batch)\n",
    "    cnn_probs = np.vstack(cnn_probs_list)\n",
    "\n",
    "    # SVM predictions (correct order: scale → PCA)\n",
    "    X_scaled = scaler.transform(X_val_feat)   # scale raw CNN features first\n",
    "    X_pca = pca.transform(X_scaled)           # then PCA\n",
    "    svm_probs = svm_model.predict_proba(X_pca)\n",
    "\n",
    "    # Fusion: average probabilities\n",
    "    fusion_probs = (cnn_probs + svm_probs) / 2\n",
    "    y_pred_fusion = np.argmax(fusion_probs, axis=1)\n",
    "    y_true = val_labels\n",
    "\n",
    "    return y_pred_fusion, y_true\n",
    "\n",
    "# ----------------------------\n",
    "# EVALUATE FUSION\n",
    "# ----------------------------\n",
    "y_pred_fusion, y_true_fusion = fusion_predict_from_features(\n",
    "    cnn_model=model,\n",
    "    svm_model=svm_model,\n",
    "    pca=pca,\n",
    "    scaler=scaler,\n",
    "    X_val_feat=X_val_feat,\n",
    "    val_labels=y_val_feat,\n",
    "    val_gen=val_gen,      \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nFusion Metrics on Validation Set:\")\n",
    "print(classification_report(y_true_fusion, y_pred_fusion))\n",
    "print(\"Cohen's kappa:\", cohen_kappa_score(y_true_fusion, y_pred_fusion))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN PIPELINE\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using dataset: {USE_DATASET}\")\n",
    "\n",
    "    # CNN TRAINING\n",
    "    print(\"\\n--- Training CNN ---\")\n",
    "    steps_per_epoch = math.ceil(train_gen.n / BATCH_SIZE)\n",
    "    validation_steps = math.ceil(val_gen.n / BATCH_SIZE)\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    model.save(FINAL_CNN_MODEL_PATH)\n",
    "    print(\"Final CNN model saved:\", FINAL_CNN_MODEL_PATH)\n",
    "\n",
    "    # FEATURE EXTRACTION\n",
    "    print(\"\\n--- Extracting CNN features ---\")\n",
    "    feat_model = feature_extractor(model)\n",
    "    feat_model.save(CNN_FEATURE_EXTRACTOR_PATH)\n",
    "    print(\"CNN feature extractor saved:\", CNN_FEATURE_EXTRACTOR_PATH)\n",
    "\n",
    "    # Reset generators for feature extraction\n",
    "    train_gen.reset()\n",
    "    val_gen.reset()\n",
    "\n",
    "    X_train_feat, y_train_feat = extract_features_in_batches(feat_model, train_gen)\n",
    "    X_val_feat, y_val_feat = extract_features_in_batches(feat_model, val_gen)\n",
    "\n",
    "    # Combine train + val features for SVM\n",
    "    X_svm = np.vstack([X_train_feat, X_val_feat])\n",
    "    y_svm = np.concatenate([y_train_feat, y_val_feat])\n",
    "    print(f\"SVM feature matrix shape: {X_svm.shape}, labels shape: {y_svm.shape}\")\n",
    "\n",
    "    # SVM TRAINING\n",
    "    print(\"\\n--- Training SVM ---\")\n",
    "    pca = PCA(n_components=min(128, X_svm.shape[1]), random_state=42)\n",
    "    X_svm_pca = pca.fit_transform(X_svm)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_svm_scaled = scaler.fit_transform(X_svm_pca)\n",
    "\n",
    "    svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', class_weight='balanced', probability=True)\n",
    "    start_time = time.time()\n",
    "    svm_model.fit(X_svm_scaled, y_svm)\n",
    "    print(f\"SVM training finished in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    y_pred = svm_model.predict(X_svm_scaled)\n",
    "    print(\"\\n--- SVM Training Metrics ---\")\n",
    "    print(classification_report(y_svm, y_pred))\n",
    "    print(\"Cohen's kappa:\", cohen_kappa_score(y_svm, y_pred))\n",
    "\n",
    "    # Save SVM artifacts\n",
    "    joblib.dump(svm_model, SVM_MODEL_PATH)\n",
    "    joblib.dump(pca, PCA_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(\"Saved SVM artifacts.\")\n",
    "\n",
    "    # OPTIONAL: FUSION PREDICTION\n",
    "    print(\"\\n--- Fusion CNN + SVM ---\")\n",
    "    y_pred_fusion, y_true_fusion = fusion_predict(\n",
    "        model,\n",
    "        svm_model,\n",
    "        pca,\n",
    "        scaler,\n",
    "        val_gen,          \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(\"\\nFusion Metrics on Validation Set:\")\n",
    "    print(classification_report(y_true_fusion, y_pred_fusion))\n",
    "    print(\"Cohen's kappa:\", cohen_kappa_score(y_val_feat, y_pred_fusion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
